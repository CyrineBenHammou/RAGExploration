{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPMvTvR5xQbLD/RWvIlRs04"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **RAG From Scratch: Query Transformations**\n","\n","Query transformations are a set of approaches focused on re-writing and / or modifying questions for retrieval."],"metadata":{"id":"gpNKUyqzqcrp"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uok-3IncqY62","executionInfo":{"status":"ok","timestamp":1708351308727,"user_tz":-60,"elapsed":40505,"user":{"displayName":"Technology Section","userId":"01781809410635652617"}},"outputId":"c7cee1db-701a-4fca-802f-2e4ff6400d09"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain_community\n","  Downloading langchain_community-0.0.20-py3-none-any.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tiktoken\n","  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langchain-openai\n","  Downloading langchain_openai-0.0.6-py3-none-any.whl (29 kB)\n","Collecting langchainhub\n","  Downloading langchainhub-0.1.14-py3-none-any.whl (3.4 kB)\n","Collecting chromadb\n","  Downloading chromadb-0.4.22-py3-none-any.whl (509 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.0/509.0 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langchain\n","  Downloading langchain-0.1.7-py3-none-any.whl (815 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.9/815.9 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.27)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.3)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n","  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n","Collecting langchain-core<0.2,>=0.1.21 (from langchain_community)\n","  Downloading langchain_core-0.1.23-py3-none-any.whl (241 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langsmith<0.1,>=0.0.83 (from langchain_community)\n","  Downloading langsmith-0.0.92-py3-none-any.whl (56 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.25.2)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.31.0)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.2.3)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n","Collecting openai<2.0.0,>=1.10.0 (from langchain-openai)\n","  Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n","  Downloading types_requests-2.31.0.20240218-py3-none-any.whl (14 kB)\n","Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.0.3)\n","Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.6.1)\n","Collecting chroma-hnswlib==0.7.3 (from chromadb)\n","  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n","  Downloading fastapi-0.109.2-py3-none-any.whl (92 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n","  Downloading uvicorn-0.27.1-py3-none-any.whl (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting posthog>=2.4.0 (from chromadb)\n","  Downloading posthog-3.4.1-py2.py3-none-any.whl (41 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.1/41.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.9.0)\n","Collecting pulsar-client>=3.1.0 (from chromadb)\n","  Downloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n","  Downloading onnxruntime-1.17.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_api-1.22.0-py3-none-any.whl (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.22.0-py3-none-any.whl (18 kB)\n","Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n","  Downloading opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl (11 kB)\n","Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_sdk-1.22.0-py3-none-any.whl (105 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.6/105.6 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.2)\n","Collecting pypika>=0.48.9 (from chromadb)\n","  Downloading PyPika-0.48.9.tar.gz (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.2)\n","Collecting overrides>=7.3.1 (from chromadb)\n","  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.1.1)\n","Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.60.1)\n","Collecting bcrypt>=4.0.1 (from chromadb)\n","  Downloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.0)\n","Collecting kubernetes>=28.1.0 (from chromadb)\n","  Downloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting mmh3>=4.0.1 (from chromadb)\n","  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n","Collecting jsonpatch<2.0,>=1.33 (from langchain)\n","  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n","Requirement already satisfied: packaging>=19.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (23.2)\n","Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.0.0)\n","Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n","  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Collecting starlette<0.37.0,>=0.36.3 (from fastapi>=0.95.2->chromadb)\n","  Downloading starlette-0.36.3-py3-none-any.whl (71 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n","  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n","Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2024.2.2)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n","Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n","Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n","Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n","Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n","Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n","Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n","Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.21->langchain_community) (3.7.1)\n","Collecting langsmith<0.1,>=0.0.83 (from langchain_community)\n","  Downloading langsmith-0.0.87-py3-none-any.whl (55 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.7.0)\n","Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.10.0->langchain-openai)\n","  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.3.0)\n","Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n","  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n","Collecting importlib-metadata<7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n","  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n","Collecting backoff<3.0.0,>=1.10.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n","Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.62.0)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.22.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.22.0-py3-none-any.whl (17 kB)\n","Collecting opentelemetry-proto==1.22.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_proto-1.22.0-py3-none-any.whl (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl (14 kB)\n","Collecting opentelemetry-instrumentation==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_instrumentation-0.43b0-py3-none-any.whl (28 kB)\n","Collecting opentelemetry-semantic-conventions==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_semantic_conventions-0.43b0-py3-none-any.whl (36 kB)\n","Collecting opentelemetry-util-http==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_util_http-0.43b0-py3-none-any.whl (6.9 kB)\n","Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n","Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n","Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading asgiref-3.7.2-py3-none-any.whl (24 kB)\n","Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n","  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.16.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.6)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n","Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.20.3)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n","Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.21->langchain_community) (1.2.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n","Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai)\n","  Downloading httpcore-1.0.3-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\n","Building wheels for collected packages: pypika\n","  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=e36967446711774f839c849869e0a97d892602bba64d7afb0a323e065b253544\n","  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n","Successfully built pypika\n","Installing collected packages: pypika, monotonic, mmh3, websockets, uvloop, types-requests, python-dotenv, pulsar-client, overrides, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, mypy-extensions, marshmallow, jsonpointer, importlib-metadata, humanfriendly, httptools, h11, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, uvicorn, typing-inspect, tiktoken, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, langchainhub, jsonpatch, httpcore, coloredlogs, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, langsmith, kubernetes, httpx, fastapi, dataclasses-json, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, openai, langchain-core, opentelemetry-instrumentation-fastapi, langchain-openai, langchain_community, langchain, chromadb\n","  Attempting uninstall: importlib-metadata\n","    Found existing installation: importlib-metadata 7.0.1\n","    Uninstalling importlib-metadata-7.0.1:\n","      Successfully uninstalled importlib-metadata-7.0.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed asgiref-3.7.2 backoff-2.2.1 bcrypt-4.1.2 chroma-hnswlib-0.7.3 chromadb-0.4.22 coloredlogs-15.0.1 dataclasses-json-0.6.4 deprecated-1.2.14 fastapi-0.109.2 h11-0.14.0 httpcore-1.0.3 httptools-0.6.1 httpx-0.26.0 humanfriendly-10.0 importlib-metadata-6.11.0 jsonpatch-1.33 jsonpointer-2.4 kubernetes-29.0.0 langchain-0.1.7 langchain-core-0.1.23 langchain-openai-0.0.6 langchain_community-0.0.20 langchainhub-0.1.14 langsmith-0.0.87 marshmallow-3.20.2 mmh3-4.1.0 monotonic-1.6 mypy-extensions-1.0.0 onnxruntime-1.17.0 openai-1.12.0 opentelemetry-api-1.22.0 opentelemetry-exporter-otlp-proto-common-1.22.0 opentelemetry-exporter-otlp-proto-grpc-1.22.0 opentelemetry-instrumentation-0.43b0 opentelemetry-instrumentation-asgi-0.43b0 opentelemetry-instrumentation-fastapi-0.43b0 opentelemetry-proto-1.22.0 opentelemetry-sdk-1.22.0 opentelemetry-semantic-conventions-0.43b0 opentelemetry-util-http-0.43b0 overrides-7.7.0 posthog-3.4.1 pulsar-client-3.4.0 pypika-0.48.9 python-dotenv-1.0.1 starlette-0.36.3 tiktoken-0.6.0 types-requests-2.31.0.20240218 typing-inspect-0.9.0 uvicorn-0.27.1 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n"]}],"source":["!pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain"]},{"cell_type":"code","source":["import os\n","os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n","os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n","os.environ['LANGCHAIN_API_KEY'] = 'Your-Key'\n","os.environ[\"OPENAI_API_KEY\"] = 'Your-Key'"],"metadata":{"id":"TfwXubr1q6CR","executionInfo":{"status":"ok","timestamp":1708351958585,"user_tz":-60,"elapsed":413,"user":{"displayName":"Technology Section","userId":"01781809410635652617"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Multi Query"],"metadata":{"id":"Wj05Fp_hrU7g"}},{"cell_type":"code","source":["#### INDEXING ####\n","\n","# Load blog\n","import bs4\n","from langchain_community.document_loaders import WebBaseLoader\n","loader = WebBaseLoader(\n","    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n","    bs_kwargs=dict(\n","        parse_only=bs4.SoupStrainer(\n","            class_=(\"post-content\", \"post-title\", \"post-header\")\n","        )\n","    ),\n",")\n","blog_docs = loader.load()\n","\n","# Split\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n","    chunk_size=300,\n","    chunk_overlap=50)\n","\n","# Make splits\n","splits = text_splitter.split_documents(blog_docs)\n","\n","# Index\n","from langchain_openai import OpenAIEmbeddings\n","from langchain_community.vectorstores import Chroma\n","vectorstore = Chroma.from_documents(documents=splits,\n","                                    embedding=OpenAIEmbeddings())\n","\n","retriever = vectorstore.as_retriever()"],"metadata":{"id":"LFZ2_cNtq_5X","executionInfo":{"status":"ok","timestamp":1708351962685,"user_tz":-60,"elapsed":2037,"user":{"displayName":"Technology Section","userId":"01781809410635652617"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### Prompt"],"metadata":{"id":"LzCCpWa-r1QQ"}},{"cell_type":"code","source":["from langchain.prompts import ChatPromptTemplate\n","\n","# Multi Query: Different Perspectives\n","template = \"\"\"You are an AI language model assistant. Your task is to generate five\n","different versions of the given user question to retrieve relevant documents from a vector\n","database. By generating multiple perspectives on the user question, your goal is to help\n","the user overcome some of the limitations of the distance-based similarity search.\n","Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n","prompt_perspectives = ChatPromptTemplate.from_template(template)\n","\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_openai import ChatOpenAI\n","\n","generate_queries = (\n","    prompt_perspectives\n","    | ChatOpenAI(temperature=0)\n","    | StrOutputParser()\n","    | (lambda x: x.split(\"\\n\"))\n",")"],"metadata":{"id":"nhCHJrTbrmqJ","executionInfo":{"status":"ok","timestamp":1708351965583,"user_tz":-60,"elapsed":264,"user":{"displayName":"Technology Section","userId":"01781809410635652617"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["from langchain.load import dumps, loads\n","\n","def get_unique_union(documents: list[list]):\n","    \"\"\" Unique union of retrieved docs \"\"\"\n","    # Flatten list of lists, and convert each Document to string\n","    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n","    # Get unique documents\n","    unique_docs = list(set(flattened_docs))\n","    # Return\n","    return [loads(doc) for doc in unique_docs]\n","\n","# Retrieve\n","question = \"What is task decomposition for LLM agents?\"\n","retrieval_chain = generate_queries | retriever.map() | get_unique_union\n","docs = retrieval_chain.invoke({\"question\":question})\n","len(docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3VFIB2Szr9WP","executionInfo":{"status":"ok","timestamp":1708351970938,"user_tz":-60,"elapsed":3261,"user":{"displayName":"Technology Section","userId":"01781809410635652617"}},"outputId":"dbea6aaf-fae4-4c19-deee-f1c1d3ea4b55"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["from operator import itemgetter\n","from langchain_openai import ChatOpenAI\n","from langchain_core.runnables import RunnablePassthrough\n","\n","# RAG\n","template = \"\"\"Answer the following question based on this context:\n","\n","{context}\n","\n","Question: {question}\n","\"\"\"\n","\n","prompt = ChatPromptTemplate.from_template(template)\n","\n","llm = ChatOpenAI(temperature=0)\n","\n","final_rag_chain = (\n","    {\"context\": retrieval_chain,\n","     \"question\": itemgetter(\"question\")}\n","    | prompt\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","final_rag_chain.invoke({\"question\":question})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"id":"ClZyMhO1sD6f","executionInfo":{"status":"ok","timestamp":1708351976519,"user_tz":-60,"elapsed":4450,"user":{"displayName":"Technology Section","userId":"01781809410635652617"}},"outputId":"44f25363-032d-4c57-f42d-68514d4db13f"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Task decomposition for LLM agents involves breaking down complex tasks into smaller and simpler steps using techniques such as Chain of Thought (CoT) and Tree of Thoughts. This process helps the agent to plan ahead and tackle the task more effectively by transforming big tasks into manageable subtasks. Task decomposition can be achieved through simple prompting, task-specific instructions, or with human inputs.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["## RAG-Fusion"],"metadata":{"id":"D5sHgSVKuDa4"}},{"cell_type":"markdown","source":["### Prompt"],"metadata":{"id":"Bxo-ss5_uMry"}},{"cell_type":"code","source":["from langchain.prompts import ChatPromptTemplate\n","\n","# RAG-Fusion: Related\n","template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n","Generate multiple search queries related to: {question} \\n\n","Output (4 queries):\"\"\"\n","prompt_rag_fusion = ChatPromptTemplate.from_template(template)"],"metadata":{"id":"1fccICmnt2Rc","executionInfo":{"status":"ok","timestamp":1708352150657,"user_tz":-60,"elapsed":308,"user":{"displayName":"Technology Section","userId":"01781809410635652617"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["from langchain_core.output_parsers import StrOutputParser\n","from langchain_openai import ChatOpenAI\n","\n","generate_queries = (\n","    prompt_rag_fusion\n","    | ChatOpenAI(temperature=0)\n","    | StrOutputParser()\n","    | (lambda x: x.split(\"\\n\"))\n",")"],"metadata":{"id":"SD82Dg4SuSaR","executionInfo":{"status":"ok","timestamp":1708352164304,"user_tz":-60,"elapsed":362,"user":{"displayName":"Technology Section","userId":"01781809410635652617"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["from langchain.load import dumps, loads\n","\n","def reciprocal_rank_fusion(results: list[list], k=60):\n","    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents\n","        and an optional parameter k used in the RRF formula \"\"\"\n","\n","    # Initialize a dictionary to hold fused scores for each unique document\n","    fused_scores = {}\n","\n","    # Iterate through each list of ranked documents\n","    for docs in results:\n","        # Iterate through each document in the list, with its rank (position in the list)\n","        for rank, doc in enumerate(docs):\n","            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n","            doc_str = dumps(doc)\n","            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n","            if doc_str not in fused_scores:\n","                fused_scores[doc_str] = 0\n","            # Retrieve the current score of the document, if any\n","            previous_score = fused_scores[doc_str]\n","            # Update the score of the document using the RRF formula: 1 / (rank + k)\n","            fused_scores[doc_str] += 1 / (rank + k)\n","\n","    # Sort the documents based on their fused scores in descending order to get the final reranked results\n","    reranked_results = [\n","        (loads(doc), score)\n","        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n","    ]\n","\n","    # Return the reranked results as a list of tuples, each containing the document and its fused score\n","    return reranked_results\n","\n","retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n","docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n","len(docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lqTfZ8qjuVkz","executionInfo":{"status":"ok","timestamp":1708352180391,"user_tz":-60,"elapsed":2220,"user":{"displayName":"Technology Section","userId":"01781809410635652617"}},"outputId":"b41050fd-61a0-4cdf-de9a-6cff2659011d"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["from langchain_core.runnables import RunnablePassthrough\n","\n","# RAG\n","template = \"\"\"Answer the following question based on this context:\n","\n","{context}\n","\n","Question: {question}\n","\"\"\"\n","\n","prompt = ChatPromptTemplate.from_template(template)\n","\n","final_rag_chain = (\n","    {\"context\": retrieval_chain_rag_fusion,\n","     \"question\": itemgetter(\"question\")}\n","    | prompt\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","final_rag_chain.invoke({\"question\":question})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"id":"I8m4hl98uZiL","executionInfo":{"status":"ok","timestamp":1708352200284,"user_tz":-60,"elapsed":4109,"user":{"displayName":"Technology Section","userId":"01781809410635652617"}},"outputId":"174f407a-f1e0-4166-ad4f-b9a3aa34acf6"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Task decomposition for LLM agents involves breaking down complex tasks into smaller and simpler steps using techniques such as Chain of Thought (CoT) and Tree of Thoughts. This allows the agent to utilize more test-time computation and enhance its performance on complex tasks by transforming big tasks into multiple manageable tasks. Task decomposition can be achieved through simple prompting, task-specific instructions, or human inputs.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["## Decomposition"],"metadata":{"id":"iKl7r-t5xGZx"}},{"cell_type":"code","source":["from langchain.prompts import ChatPromptTemplate\n","\n","# Decomposition\n","template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n","The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n","Generate multiple search queries related to: {question} \\n\n","Output (3 queries):\"\"\"\n","prompt_decomposition = ChatPromptTemplate.from_template(template)"],"metadata":{"id":"EXerD7KfxFRs","executionInfo":{"status":"ok","timestamp":1708352909983,"user_tz":-60,"elapsed":283,"user":{"displayName":"Technology Section","userId":"01781809410635652617"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers import StrOutputParser\n","\n","# LLM\n","llm = ChatOpenAI(temperature=0)\n","\n","# Chain\n","generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n","\n","# Run\n","question = \"What are the main components of an LLM-powered autonomous agent system?\"\n","questions = generate_queries_decomposition.invoke({\"question\":question})"],"metadata":{"id":"Ff_5q4pPxMCb","executionInfo":{"status":"ok","timestamp":1708352927249,"user_tz":-60,"elapsed":1944,"user":{"displayName":"Technology Section","userId":"01781809410635652617"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["questions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rPlXzZ3yxP2x","executionInfo":{"status":"ok","timestamp":1708352966582,"user_tz":-60,"elapsed":260,"user":{"displayName":"Technology Section","userId":"01781809410635652617"}},"outputId":"c8ced3e0-f359-463b-fd5a-e38a61b359f5"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['1. What is LLM technology and how does it work in autonomous agent systems?',\n"," '2. What are the specific components that make up an autonomous agent system powered by LLM?',\n"," '3. How do the main components of an LLM-powered autonomous agent system interact with each other to achieve autonomy?']"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# Prompt\n","template = \"\"\"Here is the question you need to answer:\n","\n","\\n --- \\n {question} \\n --- \\n\n","\n","Here is any available background question + answer pairs:\n","\n","\\n --- \\n {q_a_pairs} \\n --- \\n\n","\n","Here is additional context relevant to the question:\n","\n","\\n --- \\n {context} \\n --- \\n\n","\n","Use the above context and any background question + answer pairs to answer the question: \\n {question}\n","\"\"\"\n","\n","decomposition_prompt = ChatPromptTemplate.from_template(template)"],"metadata":{"id":"Ei1ZP-gMxbPB","executionInfo":{"status":"ok","timestamp":1708353003507,"user_tz":-60,"elapsed":419,"user":{"displayName":"Technology Section","userId":"01781809410635652617"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["from operator import itemgetter\n","from langchain_core.output_parsers import StrOutputParser\n","\n","def format_qa_pair(question, answer):\n","    \"\"\"Format Q and A pair\"\"\"\n","\n","    formatted_string = \"\"\n","    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n","    return formatted_string.strip()\n","\n","# llm\n","llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","\n","#\n","q_a_pairs = \"\"\n","for q in questions:\n","\n","    rag_chain = (\n","    {\"context\": itemgetter(\"question\") | retriever,\n","     \"question\": itemgetter(\"question\"),\n","     \"q_a_pairs\": itemgetter(\"q_a_pairs\")}\n","    | decomposition_prompt\n","    | llm\n","    | StrOutputParser())\n","\n","    answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n","    q_a_pair = format_qa_pair(q,answer)\n","    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair"],"metadata":{"id":"jagzrDYJxiyH","executionInfo":{"status":"ok","timestamp":1708353038844,"user_tz":-60,"elapsed":14290,"user":{"displayName":"Technology Section","userId":"01781809410635652617"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["answer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":91},"id":"3utJbKYCxteo","executionInfo":{"status":"ok","timestamp":1708353062530,"user_tz":-60,"elapsed":583,"user":{"displayName":"Technology Section","userId":"01781809410635652617"}},"outputId":"db4719c5-bf42-4f39-a677-212ac12cd9f1"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"In an LLM-powered autonomous agent system, the main components interact with each other in a coordinated manner to achieve autonomy. \\n\\n1. **Planning**: The agent breaks down complex tasks into smaller, manageable subgoals through task decomposition. This allows the agent to plan ahead and efficiently handle the steps required to accomplish the overall task.\\n\\n2. **Subgoal and Decomposition**: Task decomposition is crucial for breaking down large tasks into smaller, more manageable steps. Techniques like Chain of Thought (CoT) and Tree of Thoughts help in transforming big tasks into multiple manageable tasks, guiding the agent through a structured thinking process.\\n\\n3. **Reflection and Refinement**: After completing tasks, the agent engages in self-reflection and self-criticism over its past actions. By learning from mistakes and refining its approach for future steps, the agent continuously improves the quality of its final results. This iterative process of reflection and refinement enhances the agent's problem-solving capabilities and overall autonomy.\\n\\nOverall, the interaction between planning, subgoal decomposition, reflection, and refinement components enables the LLM-powered autonomous agent system to effectively navigate complex tasks, learn from experience, and autonomously improve its performance over time.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["## Answer individually"],"metadata":{"id":"GJZOaZwwx0xc"}},{"cell_type":"code","source":["# Answer each sub-question individually\n","\n","from langchain import hub\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_openai import ChatOpenAI\n","\n","# RAG prompt\n","prompt_rag = hub.pull(\"rlm/rag-prompt\")\n","\n","def retrieve_and_rag(question,prompt_rag,sub_question_generator_chain):\n","    \"\"\"RAG on each sub-question\"\"\"\n","\n","    # Use our decomposition /\n","    sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n","\n","    # Initialize a list to hold RAG chain results\n","    rag_results = []\n","\n","    for sub_question in sub_questions:\n","\n","        # Retrieve documents for each sub-question\n","        retrieved_docs = retriever.get_relevant_documents(sub_question)\n","\n","        # Use retrieved documents and sub-question in RAG chain\n","        answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved_docs,\n","                                                                \"question\": sub_question})\n","        rag_results.append(answer)\n","\n","    return rag_results,sub_questions\n","\n","# Wrap the retrieval and RAG process in a RunnableLambda for integration into a chain\n","answers, questions = retrieve_and_rag(question, prompt_rag, generate_queries_decomposition)"],"metadata":{"id":"7g3BHUUwx3Vo","executionInfo":{"status":"ok","timestamp":1708353116380,"user_tz":-60,"elapsed":11164,"user":{"displayName":"Technology Section","userId":"01781809410635652617"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["def format_qa_pairs(questions, answers):\n","    \"\"\"Format Q and A pairs\"\"\"\n","\n","    formatted_string = \"\"\n","    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n","        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n","    return formatted_string.strip()\n","\n","context = format_qa_pairs(questions, answers)\n","\n","# Prompt\n","template = \"\"\"Here is a set of Q+A pairs:\n","\n","{context}\n","\n","Use these to synthesize an answer to the question: {question}\n","\"\"\"\n","\n","prompt = ChatPromptTemplate.from_template(template)\n","\n","final_rag_chain = (\n","    prompt\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","final_rag_chain.invoke({\"context\":context,\"question\":question})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"id":"8V9Sxiq4yAT4","executionInfo":{"status":"ok","timestamp":1708353134203,"user_tz":-60,"elapsed":2793,"user":{"displayName":"Technology Section","userId":"01781809410635652617"}},"outputId":"bb03431b-4fc3-496d-f1dc-e85a5a9628c5"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'The main components of an LLM-powered autonomous agent system include planning, subgoal decomposition, reflection and refinement, and memory. Planning involves breaking down tasks into smaller subgoals, while reflection allows the agent to learn from past actions and refine its approach for future steps. Memory is also crucial for storing information to aid in decision-making. These components work together to enable autonomous behavior by leveraging the capabilities of LLM as the core controller in the system.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["## Step Back"],"metadata":{"id":"ZlczDcUfyOxJ"}},{"cell_type":"code","source":["# Few Shot Examples\n","from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n","examples = [\n","    {\n","        \"input\": \"Could the members of The Police perform lawful arrests?\",\n","        \"output\": \"what can the members of The Police do?\",\n","    },\n","    {\n","        \"input\": \"Jan Sindel’s was born in what country?\",\n","        \"output\": \"what is Jan Sindel’s personal history?\",\n","    },\n","]\n","# We now transform these to example messages\n","example_prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\"human\", \"{input}\"),\n","        (\"ai\", \"{output}\"),\n","    ]\n",")\n","few_shot_prompt = FewShotChatMessagePromptTemplate(\n","    example_prompt=example_prompt,\n","    examples=examples,\n",")\n","prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\n","            \"system\",\n","            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n","        ),\n","        # Few shot examples\n","        few_shot_prompt,\n","        # New question\n","        (\"user\", \"{question}\"),\n","    ]\n",")"],"metadata":{"id":"3QQXz7fkyQpQ","executionInfo":{"status":"ok","timestamp":1708353209051,"user_tz":-60,"elapsed":272,"user":{"displayName":"Technology Section","userId":"01781809410635652617"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["generate_queries_step_back = prompt | ChatOpenAI(temperature=0) | StrOutputParser()\n","question = \"What is task decomposition for LLM agents?\"\n","generate_queries_step_back.invoke({\"question\": question})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"dqO0m-CpyUmE","executionInfo":{"status":"ok","timestamp":1708353224252,"user_tz":-60,"elapsed":1123,"user":{"displayName":"Technology Section","userId":"01781809410635652617"}},"outputId":"d63af176-28bf-40ef-e43b-4a6041cca5be"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'What is the process of breaking down tasks for LLM agents?'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["# Response prompt\n","response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n","\n","# {normal_context}\n","# {step_back_context}\n","\n","# Original Question: {question}\n","# Answer:\"\"\"\n","response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n","\n","chain = (\n","    {\n","        # Retrieve context using the normal question\n","        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n","        # Retrieve context using the step-back question\n","        \"step_back_context\": generate_queries_step_back | retriever,\n","        # Pass on the question\n","        \"question\": lambda x: x[\"question\"],\n","    }\n","    | response_prompt\n","    | ChatOpenAI(temperature=0)\n","    | StrOutputParser()\n",")\n","\n","chain.invoke({\"question\": question})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":91},"id":"1baeMQFvyYc9","executionInfo":{"status":"ok","timestamp":1708353244332,"user_tz":-60,"elapsed":7073,"user":{"displayName":"Technology Section","userId":"01781809410635652617"}},"outputId":"e9726751-ebba-427b-dcad-f60567bb0a3d"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Task decomposition for LLM agents refers to the process of breaking down complex tasks into smaller, more manageable subgoals. This approach allows LLM-powered autonomous agents to efficiently handle intricate tasks by dividing them into simpler steps. Task decomposition is essential for enhancing the performance of large language models like LLMs on challenging tasks.\\n\\nOne common technique used for task decomposition is the Chain of Thought (CoT), which prompts the model to \"think step by step\" and decompose difficult tasks into smaller and simpler steps. This method transforms complex tasks into multiple manageable subtasks, providing insights into the model\\'s thinking process.\\n\\nAnother extension of CoT is the Tree of Thoughts, which explores multiple reasoning possibilities at each step of task decomposition. By generating multiple thoughts per step and creating a tree structure, this approach allows for a more comprehensive breakdown of tasks. The search process in Tree of Thoughts can be conducted using breadth-first search (BFS) or depth-first search (DFS), with each state evaluated by a classifier or majority vote.\\n\\nTask decomposition for LLM agents can be achieved through various methods, including simple prompting within the LLM model, task-specific instructions tailored to the nature of the task, or human inputs to guide the decomposition process. By breaking down tasks into smaller subgoals, LLM agents can effectively plan and execute complex tasks, leading to improved performance and problem-solving capabilities.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","source":["## HyDE"],"metadata":{"id":"qBPcfoZjyenM"}},{"cell_type":"code","source":["from langchain.prompts import ChatPromptTemplate\n","\n","# HyDE document genration\n","template = \"\"\"Please write a scientific paper passage to answer the question\n","Question: {question}\n","Passage:\"\"\"\n","prompt_hyde = ChatPromptTemplate.from_template(template)\n","\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_openai import ChatOpenAI\n","\n","generate_docs_for_retrieval = (\n","    prompt_hyde | ChatOpenAI(temperature=0) | StrOutputParser()\n",")\n","\n","# Run\n","question = \"What is task decomposition for LLM agents?\"\n","generate_docs_for_retrieval.invoke({\"question\":question})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":91},"id":"87QDbbW0yiIb","executionInfo":{"status":"ok","timestamp":1708353285056,"user_tz":-60,"elapsed":5785,"user":{"displayName":"Technology Section","userId":"01781809410635652617"}},"outputId":"2eab2bb1-46a4-457c-ac5a-f8dc9cf1e5d1"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Task decomposition is a fundamental concept in the field of reinforcement learning and artificial intelligence, particularly for Large Language Model (LLM) agents. Task decomposition refers to the process of breaking down a complex task into smaller, more manageable sub-tasks or actions that can be executed sequentially or in parallel to achieve the overall goal. \\n\\nIn the context of LLM agents, task decomposition plays a crucial role in improving the efficiency and effectiveness of the learning process. By decomposing a complex task into smaller sub-tasks, LLM agents can focus on learning and optimizing individual components before integrating them into a cohesive solution. This approach not only simplifies the learning process but also allows for better generalization and transfer of knowledge to new tasks.\\n\\nFurthermore, task decomposition enables LLM agents to leverage hierarchical reinforcement learning techniques, where higher-level policies can guide the execution of lower-level sub-tasks. This hierarchical structure helps LLM agents to learn complex tasks more efficiently by reducing the search space and providing a clear roadmap for achieving the overall goal.\\n\\nOverall, task decomposition is a critical strategy for LLM agents to tackle complex tasks effectively and efficiently, leading to improved performance and scalability in various real-world applications.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["# Retrieve\n","retrieval_chain = generate_docs_for_retrieval | retriever\n","retireved_docs = retrieval_chain.invoke({\"question\":question})\n","retireved_docs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oKChFonrymYp","executionInfo":{"status":"ok","timestamp":1708353301284,"user_tz":-60,"elapsed":5555,"user":{"displayName":"Technology Section","userId":"01781809410635652617"}},"outputId":"66148fbd-bfe4-421d-9543-cc90d7c3033b"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n"," Document(page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n"," Document(page_content='Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n"," Document(page_content='Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})]"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["# RAG\n","template = \"\"\"Answer the following question based on this context:\n","\n","{context}\n","\n","Question: {question}\n","\"\"\"\n","\n","prompt = ChatPromptTemplate.from_template(template)\n","\n","final_rag_chain = (\n","    prompt\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","final_rag_chain.invoke({\"context\":retireved_docs,\"question\":question})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"id":"r5ihi6PFywmR","executionInfo":{"status":"ok","timestamp":1708353331681,"user_tz":-60,"elapsed":1803,"user":{"displayName":"Technology Section","userId":"01781809410635652617"}},"outputId":"eb463a73-9269-4e6d-d9e8-756d8f6c9ca8"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Task decomposition for LLM agents involves breaking down complex tasks into smaller and simpler steps using techniques like Chain of Thought (CoT) and Tree of Thoughts. This process helps LLM agents manage and interpret their thinking process by transforming big tasks into multiple manageable tasks. Task decomposition can be achieved through simple prompting, task-specific instructions, or human inputs.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":29}]}]}